{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a171a6d",
   "metadata": {},
   "source": [
    "# 1.Build a NLP Language model for text generation involves train a neural network to predict the next word in a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f23f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8864da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Peter Piper picked a peck of pickled peppers.',\n",
       " 'Betty Botter bought some butter but she said the butter’s bitter.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Data\n",
    "text_data=[\"Peter Piper picked a peck of pickled peppers.\",\"Betty Botter bought some butter but she said the butter’s bitter.\"]\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d91427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f585de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create input sequences and targets\n",
    "input_sequences=[]\n",
    "\n",
    "for line in text_data:\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "for i in range(1,len(token_list)):\n",
    "    n_gram_sequence=token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "max_sequence_length=max([len(seq) for seq in input_sequences])\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "x,y=input_sequences[:,:-1],input_sequences[:,-1]\n",
    "y=tf.keras.utils.to_categorical(y,num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea8214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 2s - loss: 2.9945 - accuracy: 0.1000 - 2s/epoch - 2s/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 2.9829 - accuracy: 0.3000 - 12ms/epoch - 12ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 2.9711 - accuracy: 0.3000 - 7ms/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 2.9589 - accuracy: 0.3000 - 8ms/epoch - 8ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 2.9459 - accuracy: 0.3000 - 12ms/epoch - 12ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 2.9316 - accuracy: 0.3000 - 10ms/epoch - 10ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 2.9158 - accuracy: 0.3000 - 8ms/epoch - 8ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 2.8979 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 2.8773 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 2.8534 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 2.8253 - accuracy: 0.3000 - 10ms/epoch - 10ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 2.7920 - accuracy: 0.3000 - 8ms/epoch - 8ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 2.7521 - accuracy: 0.3000 - 7ms/epoch - 7ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 2.7044 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 2.6477 - accuracy: 0.4000 - 8ms/epoch - 8ms/step\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 2.5817 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 2.5079 - accuracy: 0.2000 - 9ms/epoch - 9ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 2.4315 - accuracy: 0.2000 - 10ms/epoch - 10ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 2.3625 - accuracy: 0.2000 - 9ms/epoch - 9ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 2.3121 - accuracy: 0.1000 - 9ms/epoch - 9ms/step\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 2.2807 - accuracy: 0.1000 - 9ms/epoch - 9ms/step\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 2.2562 - accuracy: 0.1000 - 9ms/epoch - 9ms/step\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 2.2289 - accuracy: 0.1000 - 9ms/epoch - 9ms/step\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 2.1961 - accuracy: 0.3000 - 10ms/epoch - 10ms/step\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 2.1581 - accuracy: 0.4000 - 9ms/epoch - 9ms/step\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 2.1168 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 2.0748 - accuracy: 0.3000 - 10ms/epoch - 10ms/step\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 2.0351 - accuracy: 0.3000 - 9ms/epoch - 9ms/step\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 1.9977 - accuracy: 0.4000 - 9ms/epoch - 9ms/step\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 1.9589 - accuracy: 0.4000 - 9ms/epoch - 9ms/step\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 1.9152 - accuracy: 0.4000 - 8ms/epoch - 8ms/step\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 1.8681 - accuracy: 0.4000 - 9ms/epoch - 9ms/step\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 1.8219 - accuracy: 0.3000 - 8ms/epoch - 8ms/step\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 1.7731 - accuracy: 0.4000 - 9ms/epoch - 9ms/step\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 1.7155 - accuracy: 0.5000 - 8ms/epoch - 8ms/step\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 1.6503 - accuracy: 0.6000 - 8ms/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 1.5883 - accuracy: 0.6000 - 10ms/epoch - 10ms/step\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 1.5347 - accuracy: 0.7000 - 9ms/epoch - 9ms/step\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 1.4773 - accuracy: 0.8000 - 9ms/epoch - 9ms/step\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 1.4203 - accuracy: 0.8000 - 9ms/epoch - 9ms/step\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 1.3685 - accuracy: 0.7000 - 8ms/epoch - 8ms/step\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 1.3120 - accuracy: 0.8000 - 8ms/epoch - 8ms/step\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 1.2611 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 1.2174 - accuracy: 0.9000 - 11ms/epoch - 11ms/step\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 1.1695 - accuracy: 0.7000 - 10ms/epoch - 10ms/step\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 1.1277 - accuracy: 0.7000 - 10ms/epoch - 10ms/step\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 1.0834 - accuracy: 0.9000 - 10ms/epoch - 10ms/step\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 1.0447 - accuracy: 0.9000 - 9ms/epoch - 9ms/step\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 1.0071 - accuracy: 0.9000 - 10ms/epoch - 10ms/step\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.9696 - accuracy: 0.9000 - 9ms/epoch - 9ms/step\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.9345 - accuracy: 0.9000 - 13ms/epoch - 13ms/step\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.9034 - accuracy: 0.9000 - 10ms/epoch - 10ms/step\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.8750 - accuracy: 0.9000 - 11ms/epoch - 11ms/step\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.8483 - accuracy: 0.9000 - 10ms/epoch - 10ms/step\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.8225 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.7990 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.7753 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.7544 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.7329 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.7133 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.6928 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.6735 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.6564 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.6398 - accuracy: 0.9000 - 9ms/epoch - 9ms/step\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.6246 - accuracy: 0.9000 - 8ms/epoch - 8ms/step\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.6087 - accuracy: 0.9000 - 8ms/epoch - 8ms/step\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.5932 - accuracy: 0.9000 - 9ms/epoch - 9ms/step\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.5794 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.5657 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.5523 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.5396 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.5270 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.5147 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.5028 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.4909 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.4798 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.4692 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.4586 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.4484 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.4383 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.4283 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.4188 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.4097 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.4006 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.3917 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.3832 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.3747 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.3664 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.3581 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.3501 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.3421 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.3343 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.3267 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.3191 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.3115 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.3041 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.2968 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.2896 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.2824 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.2753 - accuracy: 1.0000 - 7ms/epoch - 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2091e445290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import meta_path\n",
    "#Build the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_length-1)) # input embedd\n",
    "model.add(LSTM(100)) #hidden layer\n",
    "model.add(Dense(total_words,activation='softmax')) #output layer\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#Train\n",
    "model.fit(x,y,epochs=100,verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2506cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - 23ms/epoch - 23ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 31ms/epoch - 31ms/step\n",
      "1/1 - 0s - 25ms/epoch - 25ms/step\n",
      "1/1 - 0s - 26ms/epoch - 26ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 32ms/epoch - 32ms/step\n",
      "1/1 - 0s - 23ms/epoch - 23ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Peter Piper \n"
     ]
    }
   ],
   "source": [
    "#Generate text completion\n",
    "seed_text=\"Peter Piper\"\n",
    "next_words=20\n",
    "for _ in range(next_words):\n",
    "    token_list=tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list=tf.keras.preprocessing.sequence.pad_sequences([token_list],maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted=np.argmax(model.predict(token_list,verbose=2))\n",
    "output_word=\"\"\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if index==predicted:\n",
    "        output_word=word\n",
    "    break\n",
    "seed_text +=\" \"+output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86cae48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 22ms/epoch - 22ms/step\n",
      "1/1 - 0s - 24ms/epoch - 24ms/step\n",
      "1/1 - 0s - 26ms/epoch - 26ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 22ms/epoch - 22ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 23ms/epoch - 23ms/step\n",
      "Peter Piper picked  some butter but she said the butter’s bitter bitter bitter\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Peter Piper picked \"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=2))\n",
    "    \n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    \n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609f4914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betty Botter bought some butter but\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Betty Botter bought some butter\"\n",
    "next_words = 1\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0))  \n",
    "    \n",
    "    # Instead of appending the predicted word, create a new sequence with the predicted word\n",
    "    new_sequence = seed_text + \" \" + tokenizer.index_word[predicted]\n",
    "    \n",
    "    seed_text = new_sequence  # Update seed text for the next iteration\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756e285",
   "metadata": {},
   "source": [
    "# 2.Build a Speech to Text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0857776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4440a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "recog=sr.Recognizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "?recog.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35a599ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'adjust_for_ambient_noise',\n",
       " 'dynamic_energy_adjustment_damping',\n",
       " 'dynamic_energy_ratio',\n",
       " 'dynamic_energy_threshold',\n",
       " 'energy_threshold',\n",
       " 'lasttfgraph',\n",
       " 'listen',\n",
       " 'listen_in_background',\n",
       " 'non_speaking_duration',\n",
       " 'operation_timeout',\n",
       " 'pause_threshold',\n",
       " 'phrase_threshold',\n",
       " 'recognize_amazon',\n",
       " 'recognize_api',\n",
       " 'recognize_assemblyai',\n",
       " 'recognize_azure',\n",
       " 'recognize_bing',\n",
       " 'recognize_google',\n",
       " 'recognize_google_cloud',\n",
       " 'recognize_houndify',\n",
       " 'recognize_ibm',\n",
       " 'recognize_lex',\n",
       " 'recognize_sphinx',\n",
       " 'recognize_tensorflow',\n",
       " 'recognize_vosk',\n",
       " 'recognize_whisper',\n",
       " 'recognize_whisper_api',\n",
       " 'recognize_wit',\n",
       " 'record',\n",
       " 'snowboy_wait_for_hot_word',\n",
       " 'tflabels']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(recog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "982c0a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<speech_recognition.AudioFile at 0x209246755d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp=sr.AudioFile(\"nlp.wav\")\n",
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "080ec5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with samp as source:\n",
    "    audio=recog.record(samp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62e2c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text for the Audio:\n",
      "\n",
      "hello welcome to NLP with py\n"
     ]
    }
   ],
   "source": [
    "res=recog.recognize_google(audio)\n",
    "print('Text for the Audio:\\n')\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13486e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_to_text(file):\n",
    "    samp=sr.AudioFile(file)\n",
    "    with samp as source:\n",
    "        audio=recog.record(samp)\n",
    "    return recog.recognize_google(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f7d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope you all find the sessions useful\n"
     ]
    }
   ],
   "source": [
    "  op_text=speech_to_text(\"nlp2.wav\")\n",
    "print(op_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b370def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope', 'you', 'all', 'find', 'the', 'sessions', 'useful']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens=word_tokenize(op_text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4bcc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope', 'find', 'sessions']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list=['hope','find','sessions']\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c594457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope', 'find', 'sessions']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_features=[term for term in tokens if term in feature_list]\n",
    "review_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f093d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft Sound Mapper - Input',\n",
       " 'Microphone Array (AMD Audio Dev',\n",
       " 'Microsoft Sound Mapper - Output',\n",
       " 'Speaker (Realtek(R) Audio)',\n",
       " 'Primary Sound Capture Driver',\n",
       " 'Microphone Array (AMD Audio Device)',\n",
       " 'Primary Sound Driver',\n",
       " 'Speaker (Realtek(R) Audio)',\n",
       " 'Speaker (Realtek(R) Audio)',\n",
       " 'Microphone Array (AMD Audio Device)',\n",
       " 'Microphone Array 1 (AMDAfdInstall Wave Microphone - 0)',\n",
       " 'Microphone Array 2 (AMDAfdInstall Wave Microphone - 0)',\n",
       " 'Headphones 1 (Realtek HD Audio 2nd output with HAP)',\n",
       " 'Headphones 2 (Realtek HD Audio 2nd output with HAP)',\n",
       " 'PC Speaker (Realtek HD Audio 2nd output with HAP)',\n",
       " 'Speakers 1 (Realtek HD Audio output with HAP)',\n",
       " 'Speakers 2 (Realtek HD Audio output with HAP)',\n",
       " 'PC Speaker (Realtek HD Audio output with HAP)',\n",
       " 'Microphone (Realtek HD Audio Mic input)',\n",
       " 'Stereo Mix (Realtek HD Audio Stereo input)']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic=sr.Microphone()\n",
    "mic.list_microphone_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad1c9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mic as source:\n",
    "    audio=recog.listen(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01406d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to day 6 NLP session'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recog.recognize_google(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74480845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_features(review_text):\n",
    "    feature_list=['welcome','day','NLP']\n",
    "    tokens=word_tokenize(review_text.lower())\n",
    "    review_features=[term for term in tokens if term in feature_list]\n",
    "    review_features=list(set(review_features))\n",
    "    return review_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51f96e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['day', 'welcome']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=recog.recognize_google(audio)\n",
    "get_review_features(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a0b1d",
   "metadata": {},
   "source": [
    "# 3.Build a Text to Speech model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4a345c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User text pl >>:Hello!Welcome to NLP with python\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "def text_to_speech(text,language='en',filename='ww.mp3'):\n",
    "    tts=gTTS(text=text,lang=language,slow=False)\n",
    "    tts.save(filename)\n",
    "    os.system(f\"start {filename}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    input_text=input('User text pl >>:')\n",
    "    text_to_speech(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b0475",
   "metadata": {},
   "source": [
    "# 4.Build a NLP Language model to detect the sentence/word error in the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fe55252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f3352ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', 'a', 'powerful', 'language', 'model', 'check']\n",
      "\n",
      "['pythoon', 'i', 'A', 'power', 'langage', 'moel', 'chek']\n"
     ]
    }
   ],
   "source": [
    "#sample dataset of correctly spelled and misspelled words\n",
    "correct_words=['python','is','a','powerful','language','model','check']\n",
    "misspelled_words=['pythoon','i','A','power','langage','moel','chek']\n",
    "print(correct_words)\n",
    "print()\n",
    "print(misspelled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c11502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', 'a', 'powerful', 'language', 'model', 'check', 'pythoon', 'i', 'A', 'power', 'langage', 'moel', 'chek']\n"
     ]
    }
   ],
   "source": [
    "#combile correct & misspelled into a single dataset\n",
    "all_words=correct_words+misspelled_words\n",
    "print(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f4f4155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[1] *len(correct_words)+[0] *len(misspelled_words)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96c883d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', '', 'powerful', 'language', 'model', 'check', 'pythoon', '', '', 'power', 'langage', 'moel', 'chek']\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    text=re.sub(r'\\b\\w\\b','',text) #remove single characters\n",
    "    return text.lower()\n",
    "\n",
    "all_words=[preprocess_text(word) for word in all_words]\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f765cc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  'check',\n",
       "  'chek',\n",
       "  '',\n",
       "  'python',\n",
       "  'moel',\n",
       "  '',\n",
       "  'is',\n",
       "  'model',\n",
       "  'pythoon',\n",
       "  'power'],\n",
       " [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the dataset into train & test\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(all_words,labels,test_size=0.2,random_state=4)\n",
    "xtrain,ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0f9faee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['language', 'powerful', 'langage'], [1, 1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest,ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28e96d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 4)\t1\n",
      "  (7, 2)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 7)\t1\n",
      "  (10, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "#Vectorize the words with BOW repr\n",
    "cv=CountVectorizer()\n",
    "xtrain_cv=cv.fit_transform(xtrain)\n",
    "xtest_cv=cv.transform(xtest)\n",
    "print(xtrain_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eac86016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(xtest_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "223a1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "#Classifier model with Naive Bayes Algorithm\n",
    "clf=MultinomialNB()\n",
    "clf.fit(xtrain_cv,ytrain)\n",
    "#test\n",
    "ypred=clf.predict(xtest_cv)\n",
    "#Evaluate\n",
    "accuracy=accuracy_score(ytest,ypred)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d21d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the spell check\n",
    "def spell_check(test_word):\n",
    "    test_word_vector=cv.transform([preprocess_text(test_word)])\n",
    "    prediction=clf.predict(test_word_vector)\n",
    "    \n",
    "    if prediction[0]==1:\n",
    "\n",
    "        print(f\"{test_word} is spelled correctly.\")\n",
    "    else:\n",
    "        print(f\"{test_word} is likely misspelled \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "acfffe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is spelled correctly.\n"
     ]
    }
   ],
   "source": [
    "spell_check('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1a09885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chek is likely misspelled \n"
     ]
    }
   ],
   "source": [
    "spell_check('chek')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c0786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
